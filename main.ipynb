{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image and directory (image category)\n",
    "\n",
    "def read_image(dir_path):\n",
    "    \"\"\"\n",
    "    TBD iterator\n",
    "    \"\"\"\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            # Ignore useless files.\n",
    "            if file != \".DS_Store\":\n",
    "                yield (cv2.imread(os.path.join(subdir, file)),\n",
    "                       os.path.basename(subdir))\n",
    "                \n",
    "\n",
    "def compute_descriptors(dir_path):\n",
    "    \"\"\"\n",
    "    TBD, note on ORB instead of SIFT/SURF\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lists to contain data.\n",
    "    image_ids = []\n",
    "    descriptors = []\n",
    "    labels = []\n",
    "    \n",
    "    # Build ORB object.\n",
    "    orb = cv2.ORB_create()\n",
    "    \n",
    "    image_id = 0\n",
    "    \n",
    "    for image, label in read_image(dir_path):\n",
    "        # Detect and compute keypoints.\n",
    "        kp, des = orb.detectAndCompute(image, None)\n",
    "        # If no keypoints detected, continue.\n",
    "        if kp == []:\n",
    "            continue\n",
    "        for d in range(len(des)):\n",
    "            descriptors.append(des[d])\n",
    "            image_ids.append(image_id)\n",
    "            labels.append(label)\n",
    "            \n",
    "        image_id += 1\n",
    "            \n",
    "    des_df = pd.DataFrame(columns = ['image_id', 'descriptor', 'label'])\n",
    "    des_df['image_id'] = image_ids\n",
    "    des_df['descriptor'] = descriptors\n",
    "    des_df['label'] = labels\n",
    "    \n",
    "    \n",
    "    return des_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_words(descriptors_df, n_clusters, num_descriptors):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample `num_descriptors` descriptors.\n",
    "    data = descriptors_df['descriptor'].sample(n = num_descriptors).tolist()\n",
    "    # Compute the kmeans algorithm on the sampled descriptors.\n",
    "    kmeans = KMeans(n_clusters = n_clusters, n_jobs = -1).fit(data)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_term(word, dictionary):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    distances = [lin.norm(word - term) for term in dictionary]\n",
    "    \n",
    "    return np.argmin(distances)\n",
    "\n",
    "def compute_histogram(descriptors_df, kmeans):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    # Estract visual words (centroids) from kmeans computation.\n",
    "    words = kmeans.cluster_centers_\n",
    "    \n",
    "    descriptor_dim = len(words)\n",
    "    num_images = len(descriptors_df['image_id'])\n",
    "    \n",
    "    histograms = np.zeros((num_images, descriptor_dim))\n",
    "    \n",
    "    # For every image.\n",
    "    for index, row in descriptors_df.iterrows():\n",
    "        # For every descriptor.\n",
    "        for descriptor in row['descriptor']:\n",
    "            closest_centroid = find_nearest_term(descriptor, words)\n",
    "            histograms[index][closest_centroid] += 1\n",
    "            \n",
    "    # Compute norms and normalisation.\n",
    "    norm = np.sum(histograms, axis = 1).reshape(num_images, 1)\n",
    "    histograms = histograms / norm\n",
    "    \n",
    "    # More convenient format as list of arrays.\n",
    "    histograms = list(histograms[row] for row in range(len(histograms)))\n",
    "    \n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def nn_classifier(train_df, test_df):\n",
    "    true_labels = test_df['label']\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # For each test image.\n",
    "    for index, row in test_df.iterrows():\n",
    "        distances = []\n",
    "        # Compute distance with each train image.\n",
    "        for index, row in train_df.iterrows():\n",
    "            distances.append(wasserstein_distance(row['histogram'],\n",
    "                                                  myrow['histogram']))\n",
    "        # The predicted label corresponds to the minimum distance.\n",
    "        predicted_labels.append(train_df.iloc[np.argmin(distances)]['label'])\n",
    "        \n",
    "    return true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./dataset/train\"\n",
    "test_path = \"./dataset/test\"\n",
    "\n",
    "n_clusters = 50 # Size of dictionary.\n",
    "n_descriptors = 100000\n",
    "\n",
    "# Read images and compute descriptors, saving them in a dataframe.\n",
    "train_df = compute_descriptors(train_path)\n",
    "\n",
    "# Compute kmeans clustering using descriptors\n",
    "# Note: cluster centers are stored in this object,\n",
    "# and can be obtained using `kmeans.cluster_centers_`.\n",
    "kmeans = k_means_words(train_df, n_clusters, n_descriptors)\n",
    "\n",
    "# Aggregate descriptor info, making dataframe more compact.\n",
    "# Now the third column contains the list of descriptors.\n",
    "train_df = train_df.groupby(['image_id', 'label'],\n",
    "                            as_index = False).agg({'descriptor':\n",
    "                                                   (lambda x: list(x))})\n",
    "\n",
    "# Compute histograms and add them to dataframe.\n",
    "histograms = compute_histogram(train_df, kmeans)\n",
    "train_df['histogram'] = histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute histograms for test set, using words extracted from train.\n",
    "\n",
    "test_df = compute_descriptors(test_path)\n",
    "\n",
    "test_df = test_df.groupby(['image_id', 'label'],\n",
    "                            as_index = False).agg({'descriptor':\n",
    "                                                   (lambda x: list(x))})\n",
    "\n",
    "# Note, kmeans has not been recomputed, the training one is used.\n",
    "histograms = compute_histogram(test_df, kmeans)\n",
    "test_df['histogram'] = histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true, predicted = nn_classifier(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./dataset/train/Coast/image_0231.jpg\")\n",
    "orb = cv2.ORB_create()\n",
    "kp = orb.detect(img,None)\n",
    "kp, des = orb.compute(img, kp)\n",
    "img2 = cv2.drawKeypoints(img,kp,img)\n",
    "plt.imshow(img2),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
